
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%   Find a Nice Title: #Seen #Storytelling #Events   %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass{sig-alternate}

\usepackage{url}
\usepackage{textcomp}
\usepackage{listings}
\usepackage{color}
\usepackage{multirow}

% listing styles
\lstset{numbers=left, numberstyle=\tiny,basicstyle=\ttfamily\scriptsize, tabsize=2, keywordstyle=\underbar, stringstyle=\small, backgroundcolor=\color[gray]{0.94}, framexleftmargin=2pt}
\lstdefinestyle{rdfa}{numberblanklines=true, morekeywords={}}

% Turtle box
\definecolor{olivegreen}{rgb}{0.2,0.8,0.5}
\definecolor{grey}{rgb}{0.5,0.5,0.5}
\lstdefinelanguage{ttl}{
sensitive=true,
morecomment=[l][\color{grey}]{@},
morecomment=[l][\color{olivegreen}]{\#},
morestring=[b][\color{blue}]\",
keywordstyle=\color{cyan},
morekeywords={version,owl,rdf,rdfs,xml,xsd,dbpedia,dbo,str,sso,scms,fr,ld}
}
\lstset{
        basicstyle=\ttfamily\scriptsize,
        upquote=true,
        showspaces=false,
        showstringspaces=false,
        showtabs=false,
        tabsize=2,
        frame=none,
        breaklines,
        numbers=none,
        framexleftmargin=2mm,
        xleftmargin=2mm,
}

\newcommand{\hilight}[1]{\colorbox{yellow}{#1}}
\newcommand{\todo}[1]{\colorbox{red}{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Beginning of document  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\title{Storifying Events with Timelines of Links}

\numberofauthors{3}
\author{
\alignauthor Carlo Andrea Conte\\
	\affaddr{Mahaya Inc.}\\
    \affaddr{New York, USA}\\
    \email{carloante@msn.com}
\alignauthor Mor Naaman\\
    \affaddr{Cornell Tech}\\
    \affaddr{New York, USA}\\
    \email{mor.naaman@cornell.edu}
\alignauthor Rapha\"el Troncy\\
	\affaddr{EURECOM}\\
	\affaddr{Biot, France}\\
	\email{raphael.troncy@eurecom.fr}	\\
}

\maketitle

%%%%%%%%%%%%%%%%%%
%%%  Abstract  %%%
%%%%%%%%%%%%%%%%%%

\begin{abstract}
% Motivation: story is being told with links which are being shared. Describe the links processing algorithms, architecture, engineering. Describe the two algorithms that extract links (based on volume, based on velocity).

Social media platforms constitute a valuable source of information regarding real-world happenings. In particular, user generated contents on mobile-oriented platforms like Twitter allow for real-time narrations thanks to the instantaneous nature of their publishing. A common trend is to publish links to more exhaustive articles, media files and other types of information sources. In this paper, we will describe a system able to listen for tweets published regarding a particular event, for which a time-range and a set of specific hashtags are known. Such system will extract, resolve and eventually filter all the links according to a velocity-based function and a volume-based function. For each selected link, relevant information will be scraped from the referenced page.

\todo{Write conclusions reached}

\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.1}{Information Storage and Retrieval}{Content Analysis and Indexing}
%\terms{Algorithms,Measurement,Experimentation,Web}
\keywords{Event, Story, Content Analysis, Seen, Twitter, Hashtags, Links}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  1. Introduction  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:introduction}

\todo{Mention real-time and low-latency requirements}

\todo{we mention we only use tweets and why (see comment)}
% The reason why only tweets are taken into account is that Twitter includes in every tweet's data a list of entities. Such list contains a number of meaningful items parsed from the original text. Typical examples include hashtags and, luckily for us, web urls. Another reason for this choice is that users are more likely to publish links via Twitter than via Instagram, as the latter is a service strongly centered on content creation.

\todo{Mention we don't cover the logic for gathering tweets (see comment)}
%...as we build our experiment on top of Seen's constantly updated database. \cite Wired's article about Seen

\todo{Also mention we don't cover logic for general event metadata, we use Seen's db}

% -How people talk on Twitter about events
% -Problem of noise, quantity, quality
% -Problem of real-time computation
% -Introducing paper's sections

Twitter, Facebook, Instagram, and other social platforms provide a continuous stream of user-contributed messages and media. Very often, contents are not generated within the platform itself, but are referenced via hyperlinks. The nature of these pages can be various: they can include images, news articles, real-time video streams and many other types of content. It is possible to obtain elements related to a particular event when specific keywords and time ranges are known. Filtering the results according to their relevance, and aggregating them in a storyline, can lead to a rich narration of the event thanks to the use of a very diverse set of media.

In this document we will describe a system that aims at extracting a timeline of links from a stream of tweets. These links will be filtered in order to reduce the noise and surface valuable information. In addition to this, they will be complemented by metadata scraped from their referenced pages: this metadata can then be used to represent the timeline in an intelligible way.

After introducing similar and related projects in Section \ref{sec:related-work}, we will discuss the general architecture of the system we implemented in Section \ref{sec:architecture}. This section will also describe the implementation of the two score functions we used for our tests. Section \ref{sec:experiment} will comment the results of our experiments. Finally, in Section \ref{sec:storyline} we will see a few examples of storylines based on the results obtained in Section \ref{sec:experiment}.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  2. Related Work  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Related Work}
\label{sec:related-work}

\todo{Raphael, Mor}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  3. Architecture  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Architecture}
\label{sec:architecture}
% This section can be shortened, I'll decide when the first draft of the paper is ready and I can see how long it is

The system we propose has to be able to run in real time in order to make it possible to build storylines of events that are happening in the moment of the analysis. Because of this we also want results to be returned with the smallest delay possible since the moment raw data first entered the system. These requirements impose the adoption of an efficient, flexible concurrency model, that would be easily scalable according to the data flow. At the same time, it must be possible to process past events with the same algorithm, as results should be consistent regardless the moment the event takes place.

As we already mentioned in Section \ref{sec:introduction}, we won't cover the interface this system has with the original source of data. We will assume that a separated process keeps updating our database at regular intervals with raw data from Twitter. In our testing environment, Twitter is queried for contents for a given event about every 15 minutes, although this frequency can significantly fluctuate depending on the number of events being tracked by the system at the same time. We used a MongoDB database, where raw data was indexed according to creation time and mentioned hashtags.

The different steps implemented by such process comprehend:
\begin{enumerate}
 \item Extracting links from a collection of tweets according to the parameters identifying an event,
 \item Resolving these links to their canonical form,
 \item Selecting the most relevant links,
 \item Collecting useful data from the pages referenced by the links selected,
 \item Outputting a timeline of links . %"set" or "timeline"?
\end{enumerate}
We will now detail the general architecture of this system. Figure \ref{fig:architecture} is a graphical representation of the blocks building up the software.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{Figures/links_processing_architecture.png}
  \caption{Architecture}
  \label{fig:architecture}
\end{figure} \todo{Split this in two separate images for readability}

The complete process for each link implies two very severe bottlenecks: the url resolution and the page scraping. In order to overcome this limitation and build a more efficient system, we split the logic in different parts intended to run in parallel.

The \emph{links dispatcher} represents the first step of this processing chain. It retrieves from the \emph{content database} all the tweets for a particular temporal window and containing any of the hashtags included in a given set. After loading the raw data, the links dispatcher extracts every link from the tweets' entities (\cite{RestTweetsDoc}), it queries the \emph{links mappings} class to check if such URL has been resolved already. If it hasn't, it places it in a \emph{links resolutor queue}, otherwise it adds it together with information about its tweet to the \emph{links appearances}. The links dispatcher can be run on happening events, by computing the temporal window's length according on the time of the previous processing, but it can also process past events, by using a sliding window algorithm where the window has a fixed size.

The links resolutor queue is a Redis queue\cite{RedisQueues} of jobs. This means that a list of pointers to python functions and their arguments is stored in a Redis database for later execution. The processes responsible of reading job queues and executing them are called ``workers''. The advantages of this approach resides in the fact that many workers instances can be run at the same time, and their number and behavior can be adjusted according to the workload. This allows for great flexibility in the way the system can be scaled according to the abundancy of data.

All the urls in the links resolutor queue will be resolved by the \emph{links resolutor} function. This function will first look the url up in the links mappings and, if it is not found, it resolve the url and eventually save a new mapping. This resolution step is necessary because most links on Twitter are shortened by various url shortener services (sometimes automatically provided by Twitter itself). When a link is successfully resolved, it is added to the \emph{links appearances} and a new mapping is saved.

The \emph{links appearances} class manages an unnormalized collection of all the resolved links. Every item contains information about the url, the tweet that contained it and the event that url was for. In fact, multiple items might carry data about the same combination of url and tweet as long as they reference different event ids.

Links appearances are periodically accessed by the \emph{decider}, which is responsible of ranking them and filtering them. The computation is always done on a sliding temporal window of fixed size, and only for links of one event at a time. The scoring system relies on \emph{links score processors} for its decision process: different score processors can be implemented for testing different score functions. We will detail the links score processors used for our experiments in Sections \ref{sec:volume_based_links_selection} and \ref{sec:velocity_based_links_selection}. If a link is selected for publication, the decider queries the \emph{pages metadata} class for any available metadata for that link. If no metadata is available, the link appearance is added to the \emph{Links Metadata Queue}, otherwise it is saved together with its metadata as an \emph{event link}. The decider can either be run in real time on happening events, or it can simulate the same sliding window algorithm on past events.

The \emph{Links Metadata Queue} is processed by the \emph{metadata scraper}. This function extracts the domain from the url and selects a scraper class accordingly, it loads the referenced page and extracts pieces of information from it (e.g. a title, a description and a representative image). Different scrapers look for different tags in the DOM structure, as different websites usually expose different information in different ways. When no particular scraper is specified for a particular domain, a generic scraper is used for collecting the information stored in Open Graph and Twitter Cards meta-tags. Only links for which enough metadata is found are saved as event links while the pages metadata collection is updated accordingly. Event link duplicates are avoided, while their score is updated to the latest result.

The final results of this system are stored in the event links collection. This dataset holds all the links together with their score, their volume and the time of their first appearance (this information is relative to the decider's processing window in which they were selected with the highest score \todo{verify}). In addition to this, the same record contains the metadata extracted from the referenced page and the id of the event for which that link was processed. In fact, the same link can appear multiple times as long as it referenced different events.

/todo{How do we store events' metadata (see comment)}
%Can I talk about "event ids'' even if I haven't mentioned how we store events' metadata?

% Pull some statistics out of the links resolutor, with some nice charts of the SOURCES of this links OR a volume graph ordered by source, sources = registered domains.

% Compare the results for our domain of interest (news) with results from other types of event (concerts, conferences). I expect to see here something like a big slice of Instagrams, Vines, and then I hope a long tail of websites like the NYTimes or the CCN or spam bots for news events. Ideally we will be able to say how music events and tv shows have a huge amount of instant media and fewer articles, conferences have articles AND instant media, while news have a bunch of pictures of TVs tuned on the news channel and other quite redundant stuff, while the useful information resides in the long tail of domains (newspapers, blogs). 
% This requires a small addition, I just need to keep a count in the database of some information we already extract in the process, I’m sure it’s worth it.

\subsection{Volume based links selection}
\label{sec:volume_based_links_selection}
When the LSP\footnote{Links Score Processor} executes its filtering, only on a subset of the link appearances shared for an event is available for evaluation, as the analysis is conducted per each time-window. By counting the link appearances, the LSP computes the number of times every link has been shared in the current window, in other words their volumes. As an example, Figure \ref{fig:batkid_whitehouse_volume} shows the number of appearances in time of the link to president Obama's Vine for Batkid\footnote{An initiative of Make-A-Wish Foundation for a child affected by leukemia and that attracted the attention of social media: http://abcnews.go.com/US/batkids-make-transforming-san-francisco-gotham/story?id=20899254}.
\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{Figures/batkid_whitehouse_volume.png}
  \caption{Volume of president Obama's Vine video for Batkid shares.}
  \label{fig:batkid_whitehouse_volume}
\end{figure} \todo{Higher time-resolution excel graph}

The limited information provided by these items is complemented by the general data available for the event: start time, end time, and total number of items found for the event. These informations can be used to estimate the exceptional volume a link should have in order to be considered relevant:
\begin{center} $threshold =  w \times max[5, 3+0.12\sqrt{nitems}+0.06\sqrt{expectation-nitems}]$ with $w>1$ \end{center}
Where $threshold$ is the value against which we will compare the highlight's volume and $w$ is a weight used to adjust the selectivity.

The expectation is computed according to the portion of the event in which the current time window lays:
\[ expectation = \left\{
  \begin{array}{l l}
    nitems & \quad \textrm{if $portion$ = 1}\\
     \frac{nitems}{min[1,\log_2(1+2 \times portion)]} & \quad \textrm{otherwise}
  \end{array} \right.\]
With $nitems$ being the number of total items contained in the event, and $portion$ never being smaller than one half of the entire duration (we cannot be confident about estimations done at the beginning of an event):
\begin{center} $portion = max\{0.5, min[1, \frac{now-start}{end-start}]\}$ \end{center}

When this LSP is used by the decider, only links with a sufficiently high volume in a particular time bucket will be selected for publication.

\subsection{Velocity based links selection}
\label{sec:velocity_based_links_selection}
LSP explanation
% For each link appearance, the metadata relative to the tweet that contained it is available, and it can be used for computing score functions based on the social interactions around a particular link.
%%%%%%%%%%%%%%%%%%%%%%%%
%%%  4. Experiments  %%%
%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experiments}
\label{sec:experiment}

\subsection{Dataset}
Describe what the dataset is (Seen's dataset), how data is not already organized by event, but the events database gives us the necessary info to split it.

\subsection{Volume based LSP experimental results}

Volume graphs of some approved links, explanations, interpretation of the reasons

\subsection{Velocity based LSP experimental results}

Cross validation: In the previous two subsections we can pretty much only validate the precision of the two algorithms together. How about considering the set of elements that were not selected in one algorithm, but were selected in the other one and vice versa to see if one has a better recall?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  5. Building a Storyline  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Building a Storyline}
\label{sec:storyline}
We should place the selected links (selected based on any method we decided was better for this scope between the previous two) in chronological order and see if they actually follow the evolution of the story, with what granularity. This will only work with a long event.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  6. Conclusion and Future Work  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion and Future Work}
\label{sec:conclusions}


\section{Acknowledgments}
\label{sec:ack}

\nocite{*}
\bibliographystyle{abbrv}
\bibliography{seen}
\balancecolumns
% That's all folks!
\end{document} 
